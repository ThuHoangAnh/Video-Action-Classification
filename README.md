# ğŸ¥ Video Action Classification with VideoMAE

This project implements a **video-based human action recognition pipeline** using a **pretrained VideoMAE transformer backbone** from HuggingFace.  
It supports multi-frame input, strong data augmentation, train/validation split, early stopping, and **test-time augmentation (TTA)** with multi-clip averaging.

Designed for **Kaggle-style competitions** with CSV submission output.

---

## âœ¨ Features

- ğŸï¸ Multi-frame video input
- ğŸ¤— Pretrained VideoMAE backbone (HuggingFace)
- ğŸ§  Temporal aggregation
- ğŸ”€ Strong data augmentation
- ğŸ§ª Train / validation split
- â¹ï¸ Early stopping on best validation accuracy
- ğŸ” Test-time augmentation (multi-clip averaging)
- âš¡ Mixed precision training (AMP)
- ğŸ“„ Automatic CSV submission generation

---

## ğŸ“ Project Structure

```text
Video-Action-Classification/
â”œâ”€â”€ hmdb51_data/        # Dataset directory
â”œâ”€â”€ weights/           # Saved models
â”œâ”€â”€ LSVIT-HMDB51.ipynb # Training & inference notebook
â”œâ”€â”€ VideoMAE.ipynb     # Model experiments
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
---
```

## ğŸ§  Model

This project uses:

- **VideoMAE** (Masked Autoencoder for Video Transformers)
- Pretrained on large-scale video datasets
- Fine-tuned for action classification

Backbone source:
> https://huggingface.co/MCG-NJU/videomae-base

---

## ğŸ“¦ Installation

```bash
pip install torch torchvision torchaudio
pip install transformers accelerate tqdm opencv-python
```

## ğŸ“‚ Dataset Format

Expected structure (train):
```text
data_train/
â”œâ”€â”€ class_1/
â”‚   â””â”€â”€ video_001/
â”‚       â”œâ”€â”€ 0001.jpg
â”‚       â”œâ”€â”€ 0002.jpg
â”‚       â””â”€â”€ ...
â”œâ”€â”€ class_2/
â”‚   â””â”€â”€ ...

data_test/
â”œâ”€â”€ 00001/
â”‚   â”œâ”€â”€ 0001.jpg
â”‚   â”œâ”€â”€ 0002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 00002/
â”‚   â””â”€â”€ ...
```
## âš™ï¸ Configuration

Key parameters:

```python
NUM_FRAMES = 16
FRAME_STRIDE = 2
IMG_SIZE = 224

BATCH_SIZE = 8
EPOCHS = 20
BASE_LR = 2e-5
WEIGHT_DECAY = 0.05
GRAD_ACCUM_STEPS = 8
```
## ğŸ‹ï¸ Training

Training includes:

- Mixed precision (AMP)
- Gradient accumulation
- Cosine LR schedule
- Early stopping

### To train

Run the training cells inside the notebook:

```python
train_one_epoch(...)
```

Best model is automatically saved as:

```python
best_videomae.pt
```
## ğŸ›‘ Early Stopping

The best model is saved based on validation accuracy:
```python
if val_acc > best_acc:
    torch.save(...)
```
## ğŸ” Inference

Load best model:
```python
ckpt = torch.load("best_videomae.pt")
model.load_state_dict(ckpt["model"])
model.eval()
```
## ğŸ” Test-Time Augmentation (TTA)

TTA improves performance by averaging predictions across multiple temporal offsets.

Offsets example:
```python
offsets = (0, 4, 8)
```
Final prediction = mean of logits from all offsets.

## ğŸ§¾ Submission Format

The script generates:
submission.csv
Format:
```python
id,class
1,Walking
2,Running
3,Jumping
```
## ğŸ“Š Results
| Metric              | Score                     |
| ------------------- | ------------------------- |
| Train Accuracy      | ~0.99                     |
| Validation Accuracy | ~0.78                     |
| Public Test Score   | ~0.61 â†’ Improved with TTA |

## ğŸš€ Future Improvements

- Grouped train/val split by video ID
- Larger `NUM_FRAMES` (e.g., 32)
- More temporal offsets for TTA
- Ensemble models
- Audio fusion

---

## ğŸ“š References

- **VideoMAE**: https://arxiv.org/abs/2203.12602 â†—
- **HuggingFace Transformers**: https://huggingface.co/docs/transformers â†—

