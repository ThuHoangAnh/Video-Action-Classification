{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "frWXOGfqJ2So",
        "outputId": "bef8b9b8-38d7-48da-a1cf-0797d8d4a9c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de106e15-7f11-4aa6-b952-b22b4e70ca4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de106e15-7f11-4aa6-b952-b22b4e70ca4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"anhthuh\",\"key\":\"555b7b9c5e836efdd35e27c1d31c1ffc\"}'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa-wvoCHK8Nn",
        "outputId": "66149c34-f100-49bb-e5bc-f59ac517dbb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading action-video.zip to /content\n",
            " 99% 3.12G/3.14G [00:35<00:00, 176MB/s]\n",
            "100% 3.14G/3.14G [00:35<00:00, 94.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c action-video\n",
        "!unzip -q action-video.zip -d action-video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG7CBZAOLDJ4",
        "outputId": "55afedb3-ac7f-4121-b8ca-737be2606419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import amp\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import timm\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {DEVICE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-F5G_yILHP5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLuEaTIbLMy4",
        "outputId": "63851b80-d5ea-4c33-b02b-28c78dbf5b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data: /content/action-video/data/data_train\n",
            "Test data: /content/action-video/data/test\n",
            "Model: vit_small_patch16_224\n",
            "Frames per video: 16\n",
            "Batch size: 2\n",
            "Epochs: 4\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "PATH_DATA_TRAIN = '/content/action-video/data/data_train'\n",
        "PATH_DATA_TEST = '/content/action-video/data/test'\n",
        "\n",
        "# Model parameters\n",
        "NUM_FRAMES = 16\n",
        "FRAME_STRIDE = 2\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 4\n",
        "BASE_LR = 1e-5\n",
        "HEAD_LR = 5e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "GRAD_ACCUM_STEPS = 8\n",
        "\n",
        "PRETRAINED_NAME = 'vit_small_patch16_224'\n",
        "\n",
        "print(f\"Train data: {PATH_DATA_TRAIN}\")\n",
        "print(f\"Test data: {PATH_DATA_TEST}\")\n",
        "print(f\"Model: {PRETRAINED_NAME}\")\n",
        "print(f\"Frames per video: {NUM_FRAMES}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {EPOCHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrYOjCj3LOjP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQNPOAjcLO1a",
        "outputId": "e292d1c4-cfe2-4504-e920-85e62f9f9589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lightweight ViT defined\n",
            "  Backbone: vit_small_patch16_224\n"
          ]
        }
      ],
      "source": [
        "class LightweightViTForAction(nn.Module):\n",
        "    \"\"\"Lightweight ViT for action recognition.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=51, pretrained_name='vit_small_patch16_224'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ViT (smaller backbone)\n",
        "        self.vit = timm.create_model(pretrained_name, pretrained=True, num_classes=0)\n",
        "\n",
        "        # Get embedding dimension\n",
        "        self.embed_dim = self.vit.num_features\n",
        "\n",
        "        # Simple classification head\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, video):\n",
        "        '''\n",
        "        Args:\n",
        "            video: [B, T, C, H, W] - batch of video clips\n",
        "        Returns:\n",
        "            logits: [B, num_classes]\n",
        "        '''\n",
        "        B, T, C, H, W = video.shape\n",
        "\n",
        "        # Reshape to process all frames\n",
        "        x = video.reshape(B * T, C, H, W)\n",
        "\n",
        "        # Extract features with ViT\n",
        "        features = self.vit.forward_features(x)  # [B*T, embed_dim]\n",
        "        if features.dim() == 3:\n",
        "            features = features[:, 0]  # CLS -> (B*T, embed_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        features = features.reshape(B, T, self.embed_dim)\n",
        "\n",
        "        # Temporal pooling\n",
        "        pooled = features.mean(dim=1)  # [B, embed_dim]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.head(pooled)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"Lightweight ViT defined\")\n",
        "print(f\"  Backbone: {PRETRAINED_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj8VxynkLTYm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNUEZG7ALTzG"
      },
      "outputs": [],
      "source": [
        "class VideoTransform:\n",
        "    def __init__(self, image_size: int, is_train: bool = True):\n",
        "        self.image_size = image_size\n",
        "        self.is_train = is_train\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def __call__(self, frames: torch.Tensor) -> torch.Tensor:\n",
        "        # frames: [T, C, H, W]\n",
        "\n",
        "        if self.is_train:\n",
        "            # Random resized crop (scale 0.8-1.0)\n",
        "            h, w = frames.shape[-2:]\n",
        "            scale = random.uniform(0.8, 1.0)\n",
        "            new_h, new_w = int(h * scale), int(w * scale)\n",
        "            frames = TF.resize(frames, [new_h, new_w], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "            # Random crop to target size\n",
        "            i = random.randint(0, max(0, new_h - self.image_size))\n",
        "            j = random.randint(0, max(0, new_w - self.image_size))\n",
        "            frames = TF.crop(frames, i, j, min(self.image_size, new_h), min(self.image_size, new_w))\n",
        "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "            # Horizontal flip\n",
        "            if random.random() < 0.5:\n",
        "                frames = TF.hflip(frames)\n",
        "\n",
        "            # Color jitter (brightness, contrast, saturation) - nhẹ\n",
        "            if random.random() < 0.3:\n",
        "                brightness_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_brightness(frames, brightness_factor)\n",
        "\n",
        "            if random.random() < 0.3:\n",
        "                contrast_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_contrast(frames, contrast_factor)\n",
        "\n",
        "            if random.random() < 0.3:\n",
        "                saturation_factor = random.uniform(0.9, 1.1)\n",
        "                frames = TF.adjust_saturation(frames, saturation_factor)\n",
        "        else:\n",
        "            # Val/test: center crop\n",
        "            frames = TF.resize(frames, [self.image_size, self.image_size], interpolation=InterpolationMode.BILINEAR)\n",
        "\n",
        "        # Normalize\n",
        "        normalized = [TF.normalize(frame, self.mean, self.std) for frame in frames]\n",
        "        return torch.stack(normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F6RWUGoLXEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIn8FuRNLW0d"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Tuple, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Competition TRAIN dataset.\n",
        "\n",
        "    Expected structure:\n",
        "      root/\n",
        "        class_1/\n",
        "          video_folder_A/  (contains frames .jpg/.png)\n",
        "          video_folder_B/\n",
        "        class_2/\n",
        "          ...\n",
        "\n",
        "    Returns:\n",
        "      video: (T, C, H, W) float tensor (normalized by VideoTransform)\n",
        "      label: int\n",
        "    \"\"\"\n",
        "\n",
        "    IMG_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        num_frames: int = 16,\n",
        "        frame_stride: int = 2,\n",
        "        image_size: int = 224,\n",
        "        is_train: bool = True,\n",
        "        transform=None,  # pass VideoTransform(image_size, is_train=...)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.root = Path(root)\n",
        "        self.num_frames = int(num_frames)\n",
        "        self.frame_stride = max(1, int(frame_stride))\n",
        "        self.image_size = int(image_size)\n",
        "\n",
        "        self.transform = transform  # expects VideoTransform\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        # classes\n",
        "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
        "        if not self.classes:\n",
        "            raise RuntimeError(f\"No class folders found in: {self.root}\")\n",
        "\n",
        "        self.class_to_idx = {name: idx for idx, name in enumerate(self.classes)}\n",
        "\n",
        "        # samples: list of (frame_paths, label)\n",
        "        self.samples: List[Tuple[List[Path], int]] = []\n",
        "        for cls in self.classes:\n",
        "            cls_dir = self.root / cls\n",
        "            for video_dir in sorted([d for d in cls_dir.iterdir() if d.is_dir()]):\n",
        "                frame_paths = sorted(\n",
        "                    [p for p in video_dir.iterdir() if p.suffix.lower() in self.IMG_EXTS]\n",
        "                )\n",
        "                if frame_paths:\n",
        "                    self.samples.append((frame_paths, self.class_to_idx[cls]))\n",
        "\n",
        "        if not self.samples:\n",
        "            raise RuntimeError(f\"No videos found under: {self.root}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    # def _select_indices(self, total: int) -> torch.Tensor:\n",
        "    #     \"\"\"\n",
        "    #     HMDB-style uniform sampling:\n",
        "    #     - Build a linspace grid across [0, total-1]\n",
        "    #     - Take every frame_stride\n",
        "    #     - Pad with last index if not enough\n",
        "    #     - Return exactly num_frames indices\n",
        "    #     \"\"\"\n",
        "    #     if total <= 0:\n",
        "    #         raise ValueError(\"Video has no frames\")\n",
        "    #     if total == 1:\n",
        "    #         return torch.zeros(self.num_frames, dtype=torch.long)\n",
        "\n",
        "    #     steps = max(self.num_frames * self.frame_stride, self.num_frames)\n",
        "    #     grid = torch.linspace(0, total - 1, steps=steps)\n",
        "    #     idxs = grid[:: self.frame_stride].long()\n",
        "\n",
        "    #     if idxs.numel() < self.num_frames:\n",
        "    #         pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item())\n",
        "    #         idxs = torch.cat([idxs, pad], dim=0)\n",
        "\n",
        "    #     return idxs[: self.num_frames]\n",
        "\n",
        "    def _select_indices(self, total: int, start_offset: int = 0) -> torch.Tensor:\n",
        "    # total = number of frames in this video folder\n",
        "        if total <= 0:\n",
        "            raise ValueError(\"No frames\")\n",
        "\n",
        "        # Clamp offset so it's always valid\n",
        "        start_offset = int(max(0, min(start_offset, total - 1)))\n",
        "\n",
        "        # We want NUM_FRAMES samples with spacing FRAME_STRIDE (roughly)\n",
        "        # Compute the maximum index we can reach\n",
        "        max_needed = start_offset + (self.num_frames - 1) * self.frame_stride\n",
        "\n",
        "        if max_needed <= total - 1:\n",
        "            # Simple case: we can take a regular stride sequence\n",
        "            idxs = start_offset + torch.arange(self.num_frames) * self.frame_stride\n",
        "            return idxs.long()\n",
        "\n",
        "        # Hard case: not enough frames for this offset/stride\n",
        "        # Build as many as possible, then pad with last frame\n",
        "        idxs = start_offset + torch.arange(self.num_frames) * self.frame_stride\n",
        "        idxs = idxs.clamp(0, total - 1)  # clamp all to valid range\n",
        "        return idxs.long()\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        frame_paths, label = self.samples[idx]\n",
        "        total = len(frame_paths)\n",
        "\n",
        "        idxs = self._select_indices(total).tolist()\n",
        "\n",
        "        frames = []\n",
        "        for i in idxs:\n",
        "            path = frame_paths[i]\n",
        "            with Image.open(path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                frames.append(self.to_tensor(img))  # (C,H,W) float [0,1]\n",
        "\n",
        "        video = torch.stack(frames, dim=0)  # (T,C,H,W)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            video = self.transform(video)\n",
        "\n",
        "        return video, label\n",
        "\n",
        "\n",
        "# class TestDataset(Dataset):\n",
        "#     def __init__(self, root, num_frames=16, frame_stride=2, image_size=224, transform=None, start_offset=0):\n",
        "#         self.root = Path(root)\n",
        "#         self.num_frames = num_frames\n",
        "#         self.frame_stride = frame_stride\n",
        "#         self.transform = transform\n",
        "#         self.start_offset = start_offset\n",
        "#         self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "#         self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
        "#         self.video_ids = [int(d.name) for d in self.video_dirs]\n",
        "\n",
        "#     def _select_indices(self, total: int):\n",
        "#         if total <= 0:\n",
        "#             raise ValueError(\"No frames\")\n",
        "\n",
        "#         # total frames needed in timeline\n",
        "#         needed = self.num_frames * self.frame_stride\n",
        "\n",
        "#         # choose a start position that shifts the sampling window\n",
        "#         # clamp so it stays inside [0, total-needed]\n",
        "#         max_start = max(0, total - needed)\n",
        "#         start = min(self.start_offset, max_start)\n",
        "\n",
        "#         # indices: start, start+stride, start+2*stride, ...\n",
        "#         idxs = torch.arange(start, start + needed, step=self.frame_stride, dtype=torch.long)\n",
        "#         if idxs.numel() < self.num_frames:\n",
        "#             # pad if video too short\n",
        "#             pad = idxs.new_full((self.num_frames - idxs.numel(),), idxs[-1].item() if idxs.numel() else 0)\n",
        "#             idxs = torch.cat([idxs, pad], dim=0)\n",
        "#         return idxs[:self.num_frames]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.video_dirs)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         video_dir = self.video_dirs[idx]\n",
        "#         video_id = self.video_ids[idx]\n",
        "#         frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
        "#         total = len(frame_paths)\n",
        "#         idxs = self._select_indices(total)\n",
        "\n",
        "#         frames = []\n",
        "#         for i in idxs:\n",
        "#             path = frame_paths[int(i.item())]\n",
        "#             with Image.open(path) as img:\n",
        "#                 img = img.convert(\"RGB\")\n",
        "#             frames.append(self.to_tensor(img))  # [C,H,W]\n",
        "#         video = torch.stack(frames)            # [T,C,H,W]\n",
        "#         if self.transform:\n",
        "#             video = self.transform(video)\n",
        "#         return video, video_id\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, root, num_frames=16, frame_stride=2, image_size=224, transform=None, start_offset=0):\n",
        "        self.root = Path(root)\n",
        "        self.num_frames = num_frames\n",
        "        self.frame_stride = frame_stride\n",
        "        self.transform = transform or VideoTransform(image_size=image_size, is_train=False)\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.start_offset = int(start_offset)  # <-- add this\n",
        "\n",
        "        self.video_dirs = sorted([d for d in self.root.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
        "        self.video_ids = [int(d.name) for d in self.video_dirs]\n",
        "\n",
        "    def _select_indices(self, total: int, start_offset: int = 0) -> torch.Tensor:\n",
        "    # total = number of frames in this video folder\n",
        "        if total <= 0:\n",
        "            raise ValueError(\"No frames\")\n",
        "\n",
        "        # Clamp offset so it's always valid\n",
        "        start_offset = int(max(0, min(start_offset, total - 1)))\n",
        "\n",
        "        # We want NUM_FRAMES samples with spacing FRAME_STRIDE (roughly)\n",
        "        # Compute the maximum index we can reach\n",
        "        max_needed = start_offset + (self.num_frames - 1) * self.frame_stride\n",
        "\n",
        "        if max_needed <= total - 1:\n",
        "            # Simple case: we can take a regular stride sequence\n",
        "            idxs = start_offset + torch.arange(self.num_frames) * self.frame_stride\n",
        "            return idxs.long()\n",
        "\n",
        "        # Hard case: not enough frames for this offset/stride\n",
        "        # Build as many as possible, then pad with last frame\n",
        "        idxs = start_offset + torch.arange(self.num_frames) * self.frame_stride\n",
        "        idxs = idxs.clamp(0, total - 1)  # clamp all to valid range\n",
        "        return idxs.long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_dirs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_dir = self.video_dirs[idx]\n",
        "        video_id = self.video_ids[idx]\n",
        "\n",
        "        frame_paths = sorted([p for p in video_dir.iterdir() if p.suffix.lower() in (\".jpg\", \".jpeg\", \".png\")])\n",
        "        total = len(frame_paths)\n",
        "\n",
        "        idxs = self._select_indices(total, start_offset=self.start_offset)\n",
        "\n",
        "        frames = []\n",
        "        for i in idxs:\n",
        "            path = frame_paths[int(i.item())]\n",
        "            with Image.open(path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                frames.append(self.to_tensor(img))\n",
        "\n",
        "        video = torch.stack(frames)          # [T,C,H,W]\n",
        "        video = self.transform(video)        # normalize etc\n",
        "        return video, video_id\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Train batch: (video, int_label) -> labels tensor long\n",
        "    Test batch:  (video, video_id)  -> ids list\n",
        "    \"\"\"\n",
        "    videos = torch.stack([b[0] for b in batch], dim=0)  # (B,T,C,H,W)\n",
        "    second = [b[1] for b in batch]\n",
        "    if isinstance(second[0], int):\n",
        "        second = torch.tensor(second, dtype=torch.long)\n",
        "    return videos, second\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF-iExeGLasc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrkJF1B-Lajw",
        "outputId": "98f86d2c-93eb-493e-b922-4fe175372bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train clips: 5600 | Val clips: 654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "TRAIN_ROOT = \"/content/action-video/data/data_train\"\n",
        "# If you don’t have a real val folder, you must split (see note below)\n",
        "TEST_ROOT  = \"/content/action-video/data/test\"\n",
        "\n",
        "NUM_FRAMES = 16\n",
        "FRAME_STRIDE = 2\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "train_tf = VideoTransform(image_size=IMG_SIZE, is_train=True)\n",
        "val_tf   = VideoTransform(image_size=IMG_SIZE, is_train=False)\n",
        "full_dataset = VideoDataset(TRAIN_ROOT, NUM_FRAMES, FRAME_STRIDE, IMG_SIZE, is_train=True, transform=train_tf)\n",
        "\n",
        "\n",
        "def base_video_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Make a stable 'group id' for a video.\n",
        "    Examples:\n",
        "      \"jump_001\" -> \"jump\"\n",
        "      \"jump-001\" -> \"jump\"\n",
        "      \"jump001\"  -> \"jump001\" (unchanged)\n",
        "    \"\"\"\n",
        "    name = Path(name).name\n",
        "    return re.sub(r'([_-])\\d+$', '', name)\n",
        "\n",
        "def make_group_ids_from_dataset(ds):\n",
        "    \"\"\"\n",
        "    Build group ids aligned with ds.samples.\n",
        "    We reconstruct the group id from the parent folder name of the first frame path.\n",
        "    If your dataset already stores video_dir or id, replace this logic with that field.\n",
        "    \"\"\"\n",
        "    group_ids = []\n",
        "    for frame_paths, label in ds.samples:\n",
        "        # frame_paths: List[Path] or List[str] for frames in one clip/video folder\n",
        "        p0 = Path(frame_paths[0])\n",
        "        video_dir = p0.parent          # .../class_name/<video_dir>/frame001.jpg\n",
        "        gid = (label, base_video_name(video_dir.name))  # include class to avoid cross-class collision\n",
        "        group_ids.append(gid)\n",
        "    return group_ids\n",
        "\n",
        "def group_split_indices(ds, val_ratio=0.1, seed=42):\n",
        "    group_ids = make_group_ids_from_dataset(ds)\n",
        "\n",
        "    # group -> list of indices\n",
        "    buckets = defaultdict(list)\n",
        "    for i, g in enumerate(group_ids):\n",
        "        buckets[g].append(i)\n",
        "\n",
        "    groups = list(buckets.keys())\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(groups)\n",
        "\n",
        "    n_val_groups = max(1, int(len(groups) * val_ratio))\n",
        "    val_groups = set(groups[:n_val_groups])\n",
        "\n",
        "    train_idx, val_idx = [], []\n",
        "    for g, idxs in buckets.items():\n",
        "        if g in val_groups:\n",
        "            val_idx.extend(idxs)\n",
        "        else:\n",
        "            train_idx.extend(idxs)\n",
        "\n",
        "    return train_idx, val_idx\n",
        "\n",
        "# -------------------------\n",
        "# Build ONE dataset to scan samples (no leakage)\n",
        "# -------------------------\n",
        "full_scan = VideoDataset(\n",
        "    TRAIN_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    is_train=True,\n",
        "    transform=None,   # IMPORTANT: don't bake train transform here\n",
        ")\n",
        "\n",
        "train_idx, val_idx = group_split_indices(full_scan, val_ratio=0.1, seed=42)\n",
        "\n",
        "# -------------------------\n",
        "# Create TWO separate dataset objects (so transforms don’t clash)\n",
        "# -------------------------\n",
        "train_dataset = VideoDataset(\n",
        "    TRAIN_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    is_train=True,\n",
        "    transform=train_tf,\n",
        ")\n",
        "val_dataset = VideoDataset(\n",
        "    TRAIN_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    is_train=False,\n",
        "    transform=val_tf,\n",
        ")\n",
        "\n",
        "# Copy shared metadata + subset samples\n",
        "train_dataset.classes = full_scan.classes\n",
        "train_dataset.class_to_idx = full_scan.class_to_idx\n",
        "val_dataset.classes = full_scan.classes\n",
        "val_dataset.class_to_idx = full_scan.class_to_idx\n",
        "\n",
        "train_dataset.samples = [full_scan.samples[i] for i in train_idx]\n",
        "val_dataset.samples   = [full_scan.samples[i] for i in val_idx]\n",
        "\n",
        "# -------------------------\n",
        "# Dataloaders\n",
        "# -------------------------\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"Train clips: {len(train_dataset)} | Val clips: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG7wCyMpLkjQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i4TXM5ALkbK",
        "outputId": "70b5d7c8-9315-4ba7-f60f-2e63fd2af79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test clips: 510 | Test batches: 64\n"
          ]
        }
      ],
      "source": [
        "TEST_ROOT = \"/content/action-video/data/test\"\n",
        "\n",
        "test_tf = VideoTransform(image_size=IMG_SIZE, is_train=False)\n",
        "\n",
        "test_dataset = TestDataset(\n",
        "    root=TEST_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    transform=test_tf\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Test clips: {len(test_dataset)} | Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU-EZiW5LxJh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti9lGzq4LxF3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# If not installed in your environment:\n",
        "# !pip -q install transformers accelerate\n",
        "\n",
        "from transformers import VideoMAEForVideoClassification\n",
        "\n",
        "class VideoMAEAction(nn.Module):\n",
        "    \"\"\"\n",
        "    Video-pretrained backbone -> best accuracy baseline.\n",
        "    Expects video tensor: [B, T, C, H, W] float32 normalized with ImageNet mean/std.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int, pretrained_id: str = \"MCG-NJU/videomae-base\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # ignore_mismatched_sizes=True lets HF replace classification head for your num_classes\n",
        "        self.model = VideoMAEForVideoClassification.from_pretrained(\n",
        "            pretrained_id,\n",
        "            num_labels=num_classes,\n",
        "            ignore_mismatched_sizes=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, video: torch.Tensor) -> torch.Tensor:\n",
        "        # video: [B, T, C, H, W]\n",
        "        out = self.model(pixel_values=video)\n",
        "        return out.logits  # [B, num_classes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TRkTrNDL3O6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUnjCaKJL3IA"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "EPOCHS = 20\n",
        "BASE_LR = 1e-5            # good starting point for pretrained video transformer\n",
        "WEIGHT_DECAY = 0.05\n",
        "WARMUP_RATIO = 0.1\n",
        "GRAD_ACCUM_STEPS = 4\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# total training steps\n",
        "train_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "total_steps = EPOCHS * train_steps_per_epoch\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type == \"cuda\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w3NimzYL80z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnC4F2PeL8s2"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, scheduler, scaler, device, grad_accum_steps=1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
        "\n",
        "    for step, (videos, labels) in enumerate(pbar):\n",
        "        videos = videos.to(device, non_blocking=True)   # [B,T,C,H,W]\n",
        "        labels = labels.to(device, non_blocking=True)   # [B]\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
        "            logits = model(videos)                      # [B,num_classes]\n",
        "            loss = F.cross_entropy(logits, labels, label_smoothing=0.1)\n",
        "            loss = loss / grad_accum_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # metrics (use the unscaled loss for logging)\n",
        "        with torch.no_grad():\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            total_loss += (loss.item() * grad_accum_steps) * labels.size(0)\n",
        "\n",
        "        if (step + 1) % grad_accum_steps == 0 or (step + 1) == len(loader):\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            scheduler.step()\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{(total_loss/max(total,1)):.4f}\", acc=f\"{(correct/max(total,1)):.4f}\")\n",
        "\n",
        "    return total_loss / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
        "    for videos, labels in pbar:\n",
        "        videos = videos.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(videos)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        pbar.set_postfix(loss=f\"{(total_loss/max(total,1)):.4f}\", acc=f\"{(correct/max(total,1)):.4f}\")\n",
        "\n",
        "    return total_loss / max(total, 1), correct / max(total, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7deHCZSTMAIG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "571a63a750034d13b51f0877e5b18329",
            "52bf8d766f884fb7850dce8781f212ae",
            "c1333139d5364c3e8bc3af2db8e215e4",
            "3d31f66980bc40ed84689f7a64c57e92",
            "ebad0d97d7bf48d4b9cf4f74050dcf33",
            "441239379b884160baf8308bd78ef485",
            "6ee57893522d48d698da548f93a25ec3",
            "85e9d71b02f049bc9c53fab95d4baa36",
            "639fa35f227d412e85ae938b742fb8b3",
            "26bf60a97cfe4b3c8e3b549da466710b",
            "30b545443f4246d984401dcef8d1e038",
            "bbf46f9f7d6845a6abedad4349820bd3",
            "4d8c0c60a7904d56aa240db83d2d33b3",
            "b323a7118c1f454298ee83a982626c8f",
            "d121fbf04a634ae4a9719bea703f841e",
            "9f10a8e7ecfb47089c12f1033d511af0",
            "890a9b246e2e4e44a4a7d72f339d992b",
            "bfa6aa9dea8245c6bdad25e9aea4c0ec",
            "8d0e3c8a415044b4b83216b90f3c7f41",
            "9ac80028ce244c49b22ce171ba9d89e9",
            "6ccf9677fed24d10bab2ecd8d75a56a5",
            "23c236739ab3462ab3fee4c412df6814"
          ]
        },
        "id": "sZsm1CDtMAFK",
        "outputId": "e884d3a3-b50e-45eb-ec76-dd64860368f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "571a63a750034d13b51f0877e5b18329",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbf46f9f7d6845a6abedad4349820bd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "num_classes = len(full_dataset.classes)   # or train_dataset.dataset.classes\n",
        "model = VideoMAEAction(num_classes=num_classes, pretrained_id=\"MCG-NJU/videomae-base\").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "train_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "total_steps = EPOCHS * train_steps_per_epoch\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type == \"cuda\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jDReO9MDW7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDCurfS6MDR5",
        "outputId": "4d3a9571-dbb4-4dc0-96a2-8c8f6a51c5e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# ---- model ----\n",
        "num_classes = len(full_dataset.classes)  # or len(train_dataset.dataset.classes) if using random_split\n",
        "model = VideoMAEAction(num_classes=num_classes, pretrained_id=\"MCG-NJU/videomae-base\").to(DEVICE)\n",
        "\n",
        "# ---- optimizer (recommended: correct weight decay handling) ----\n",
        "decay, no_decay = [], []\n",
        "for n, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    if p.ndim == 1 or n.endswith(\".bias\") or \"layernorm\" in n.lower() or \"layer_norm\" in n.lower():\n",
        "        no_decay.append(p)\n",
        "    else:\n",
        "        decay.append(p)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [{\"params\": decay, \"weight_decay\": WEIGHT_DECAY},\n",
        "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
        "    lr=BASE_LR\n",
        ")\n",
        "\n",
        "# ---- scheduler ----\n",
        "train_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
        "total_steps = EPOCHS * train_steps_per_epoch\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type == \"cuda\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXYpfhkRMHW_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38J5Ag6bMHUD",
        "outputId": "e3661a61-5d74-48bc-d5d3-3db2ef26cdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTrain:   0%|          | 0/700 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1903696469.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.9426 | Train Acc: 0.0371\n",
            "Val   Loss: 3.8761 | Val   Acc: 0.0795\n",
            "✅ Best model saved (val_acc=0.0795) -> best_videomae.pt\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6839 | Train Acc: 0.1030\n",
            "Val   Loss: 3.5423 | Val   Acc: 0.1208\n",
            "✅ Best model saved (val_acc=0.1208) -> best_videomae.pt\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.1695 | Train Acc: 0.2409\n",
            "Val   Loss: 2.9443 | Val   Acc: 0.2584\n",
            "✅ Best model saved (val_acc=0.2584) -> best_videomae.pt\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.6346 | Train Acc: 0.4443\n",
            "Val   Loss: 2.5201 | Val   Acc: 0.3807\n",
            "✅ Best model saved (val_acc=0.3807) -> best_videomae.pt\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.2279 | Train Acc: 0.5682\n",
            "Val   Loss: 2.1545 | Val   Acc: 0.4771\n",
            "✅ Best model saved (val_acc=0.4771) -> best_videomae.pt\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.9251 | Train Acc: 0.6589\n",
            "Val   Loss: 1.9568 | Val   Acc: 0.5214\n",
            "✅ Best model saved (val_acc=0.5214) -> best_videomae.pt\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6988 | Train Acc: 0.7221\n",
            "Val   Loss: 1.7837 | Val   Acc: 0.5627\n",
            "✅ Best model saved (val_acc=0.5627) -> best_videomae.pt\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.5395 | Train Acc: 0.7654\n",
            "Val   Loss: 1.7681 | Val   Acc: 0.5398\n",
            "No improvement. patience 1/5\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.4003 | Train Acc: 0.8159\n",
            "Val   Loss: 1.6406 | Val   Acc: 0.5642\n",
            "✅ Best model saved (val_acc=0.5642) -> best_videomae.pt\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.2981 | Train Acc: 0.8384\n",
            "Val   Loss: 1.5347 | Val   Acc: 0.6070\n",
            "✅ Best model saved (val_acc=0.6070) -> best_videomae.pt\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.2165 | Train Acc: 0.8670\n",
            "Val   Loss: 1.5094 | Val   Acc: 0.5948\n",
            "No improvement. patience 1/5\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1443 | Train Acc: 0.8914\n",
            "Val   Loss: 1.4833 | Val   Acc: 0.6208\n",
            "✅ Best model saved (val_acc=0.6208) -> best_videomae.pt\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.0923 | Train Acc: 0.9118\n",
            "Val   Loss: 1.4933 | Val   Acc: 0.6162\n",
            "No improvement. patience 1/5\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.0570 | Train Acc: 0.9173\n",
            "Val   Loss: 1.4854 | Val   Acc: 0.6070\n",
            "No improvement. patience 2/5\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.0192 | Train Acc: 0.9268\n",
            "Val   Loss: 1.4565 | Val   Acc: 0.6070\n",
            "No improvement. patience 3/5\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9929 | Train Acc: 0.9380\n",
            "Val   Loss: 1.4674 | Val   Acc: 0.6086\n",
            "No improvement. patience 4/5\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9843 | Train Acc: 0.9402\n",
            "Val   Loss: 1.4797 | Val   Acc: 0.6116\n",
            "No improvement. patience 5/5\n",
            "🛑 Early stopping at epoch 17. Best val_acc=0.6208 (epoch 12)\n",
            "\n",
            "----------------------------------------\n",
            "Training completed! Best val accuracy: 0.6208 (epoch 12)\n",
            "Best model saved to: best_videomae.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "best_acc = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "checkpoint_path = Path(\"best_videomae.pt\")\n",
        "\n",
        "# early stopping settings\n",
        "patience = 5       # stop after 5 epochs with no improvement\n",
        "min_delta = 1e-4   # require at least this improvement\n",
        "wait = 0\n",
        "\n",
        "# classes list (important: keep same mapping used by dataset labels)\n",
        "classes = full_dataset.classes  # if you used random_split, still OK because both subsets share same .dataset\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        model, train_loader, optimizer, scheduler, scaler, DEVICE, grad_accum_steps=GRAD_ACCUM_STEPS\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc = evaluate(model, val_loader, DEVICE)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # ---- save best + early stop ----\n",
        "    if val_acc > best_acc + min_delta:\n",
        "        best_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "        wait = 0\n",
        "\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"classes\": classes,\n",
        "                \"best_acc\": best_acc,\n",
        "                \"epoch\": epoch + 1,\n",
        "                # optional but useful (uncomment if you want resume training)\n",
        "                # \"optimizer\": optimizer.state_dict(),\n",
        "                # \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
        "                # \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
        "            },\n",
        "            checkpoint_path,\n",
        "        )\n",
        "        print(f\"✅ Best model saved (val_acc={best_acc:.4f}) -> {checkpoint_path}\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        print(f\"No improvement. patience {wait}/{patience}\")\n",
        "\n",
        "        if wait >= patience:\n",
        "            print(f\"🛑 Early stopping at epoch {epoch+1}. Best val_acc={best_acc:.4f} (epoch {best_epoch+1})\")\n",
        "            break\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(f\"Training completed! Best val accuracy: {best_acc:.4f} (epoch {best_epoch+1})\")\n",
        "print(f\"Best model saved to: {checkpoint_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VckCVRe8MSnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHx1g8fNMShQ",
        "outputId": "8d53d414-70fb-4206-a0bd-5b0778a59817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFERENCE ON TEST SET\n",
            "Loading checkpoint from best_videomae.pt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded (best val acc: 0.6208)\n",
            "\n",
            "Loading test dataset...\n",
            "Test clips: 510 | Test batches: 64\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# FINAL INFERENCE ON TEST SET (VideoMAE)\n",
        "# =========================\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---- MUST EXIST from your previous cells ----\n",
        "# DEVICE, IMG_SIZE, NUM_FRAMES, FRAME_STRIDE, BATCH_SIZE, NUM_WORKERS, TEST_ROOT\n",
        "# VideoMAEAction (your model wrapper)\n",
        "# TestDataset (returns: (video_tensor[T,C,H,W], video_id:int))\n",
        "# VideoTransform (with is_train=False)\n",
        "# --------------------------------------------\n",
        "\n",
        "# ✅ Final collate_fn (works for train OR test)\n",
        "def collate_fn(batch):\n",
        "    videos = torch.stack([b[0] for b in batch], dim=0)   # [B,T,C,H,W]\n",
        "    second = [b[1] for b in batch]                       # label:int OR video_id:int\n",
        "    if isinstance(second[0], int):\n",
        "        second = torch.tensor(second, dtype=torch.long)  # [B]\n",
        "    return videos, second\n",
        "\n",
        "print(\"INFERENCE ON TEST SET\")\n",
        "\n",
        "checkpoint_path = Path(\"best_videomae.pt\")   # <-- must match what you saved\n",
        "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "\n",
        "ckpt = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "classes = ckpt[\"classes\"]                    # list[str]\n",
        "num_classes = len(classes)\n",
        "\n",
        "# rebuild SAME model as training\n",
        "model = VideoMAEAction(num_classes=num_classes, pretrained_id=\"MCG-NJU/videomae-base\")\n",
        "model.load_state_dict(ckpt[\"model\"], strict=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded (best val acc: {ckpt['best_acc']:.4f})\")\n",
        "\n",
        "# build test loader\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_tf = VideoTransform(image_size=IMG_SIZE, is_train=False)\n",
        "\n",
        "test_dataset = TestDataset(\n",
        "    root=TEST_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    image_size=IMG_SIZE,\n",
        "    transform=test_tf\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Test clips: {len(test_dataset)} | Test batches: {len(test_loader)}\")\n",
        "\n",
        "\n",
        "# run inference\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_test_tta(\n",
        "    model,\n",
        "    classes,                 # list[str]\n",
        "    test_root,\n",
        "    num_frames,\n",
        "    frame_stride,\n",
        "    img_size,\n",
        "    batch_size,\n",
        "    num_workers,\n",
        "    device,\n",
        "    offsets=(0, 4, 8),       # try (0, 4, 8) or (0, 2, 4, 6)\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    # build ONE transform (val/test)\n",
        "    test_tf = VideoTransform(image_size=img_size, is_train=False)\n",
        "\n",
        "    logits_sum = None\n",
        "    video_ids_ref = None\n",
        "\n",
        "    for off in offsets:\n",
        "        ds = TestDataset(\n",
        "            root=test_root,\n",
        "            num_frames=num_frames,\n",
        "            frame_stride=frame_stride,\n",
        "            image_size=img_size,\n",
        "            transform=test_tf,\n",
        "            start_offset=off,\n",
        "        )\n",
        "\n",
        "        loader = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=True,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "        all_logits = []\n",
        "        all_ids = []\n",
        "\n",
        "        for videos, video_ids in tqdm(loader, desc=f\"Infer offset={off}\", leave=False):\n",
        "            videos = videos.to(device, non_blocking=True)\n",
        "            logits = model(videos)  # [B,num_classes]\n",
        "            all_logits.append(logits.float().cpu())\n",
        "            all_ids.append(video_ids.cpu())\n",
        "\n",
        "        all_logits = torch.cat(all_logits, dim=0)  # [N,num_classes]\n",
        "        all_ids = torch.cat(all_ids, dim=0)        # [N]\n",
        "\n",
        "        # ensure consistent ordering across offsets\n",
        "        if video_ids_ref is None:\n",
        "            video_ids_ref = all_ids\n",
        "        else:\n",
        "            if not torch.equal(video_ids_ref, all_ids):\n",
        "                raise RuntimeError(\"Video id order mismatch across offsets. Check sorting in TestDataset.\")\n",
        "\n",
        "        logits_sum = all_logits if logits_sum is None else (logits_sum + all_logits)\n",
        "\n",
        "    logits_avg = logits_sum / float(len(offsets))\n",
        "    pred_idx = logits_avg.argmax(dim=1)  # [N]\n",
        "\n",
        "    # build list[(video_id, class_name)]\n",
        "    preds = []\n",
        "    for vid, pi in zip(video_ids_ref.tolist(), pred_idx.tolist()):\n",
        "        preds.append((int(vid), classes[int(pi)]))\n",
        "\n",
        "    preds.sort(key=lambda x: x[0])\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGYWD4b4MTnN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG-uXfAvMThe",
        "outputId": "c38dd5be-6611-4eaa-b7ca-821db2833e91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                               "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total predictions: 510\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "predictions = predict_test_tta(\n",
        "    model=model,\n",
        "    classes=classes,\n",
        "    test_root=TEST_ROOT,\n",
        "    num_frames=NUM_FRAMES,\n",
        "    frame_stride=FRAME_STRIDE,\n",
        "    img_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    device=DEVICE,\n",
        "    offsets=(0, 4, 8),\n",
        ")\n",
        "print(\"Total predictions:\", len(predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtqMYbIVMWi8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNlSkMwRMWgP",
        "outputId": "45c1cb0f-defd-4cef-8070-d049ff92ca43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission saved to: /content/submission.csv\n"
          ]
        }
      ],
      "source": [
        "# save submission.csv\n",
        "submission_path = Path(\"submission.csv\")\n",
        "with open(submission_path, \"w\") as f:\n",
        "    f.write(\"id,class\\n\")\n",
        "    for video_id, pred_class in predictions:\n",
        "        f.write(f\"{video_id},{pred_class}\\n\")\n",
        "\n",
        "print(f\"Submission saved to: {submission_path.resolve()}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23c236739ab3462ab3fee4c412df6814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26bf60a97cfe4b3c8e3b549da466710b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b545443f4246d984401dcef8d1e038": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d31f66980bc40ed84689f7a64c57e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bf60a97cfe4b3c8e3b549da466710b",
            "placeholder": "​",
            "style": "IPY_MODEL_30b545443f4246d984401dcef8d1e038",
            "value": " 725/725 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "441239379b884160baf8308bd78ef485": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8c0c60a7904d56aa240db83d2d33b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890a9b246e2e4e44a4a7d72f339d992b",
            "placeholder": "​",
            "style": "IPY_MODEL_bfa6aa9dea8245c6bdad25e9aea4c0ec",
            "value": "model.safetensors: 100%"
          }
        },
        "52bf8d766f884fb7850dce8781f212ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441239379b884160baf8308bd78ef485",
            "placeholder": "​",
            "style": "IPY_MODEL_6ee57893522d48d698da548f93a25ec3",
            "value": "config.json: 100%"
          }
        },
        "571a63a750034d13b51f0877e5b18329": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52bf8d766f884fb7850dce8781f212ae",
              "IPY_MODEL_c1333139d5364c3e8bc3af2db8e215e4",
              "IPY_MODEL_3d31f66980bc40ed84689f7a64c57e92"
            ],
            "layout": "IPY_MODEL_ebad0d97d7bf48d4b9cf4f74050dcf33"
          }
        },
        "639fa35f227d412e85ae938b742fb8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ccf9677fed24d10bab2ecd8d75a56a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee57893522d48d698da548f93a25ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e9d71b02f049bc9c53fab95d4baa36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890a9b246e2e4e44a4a7d72f339d992b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d0e3c8a415044b4b83216b90f3c7f41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac80028ce244c49b22ce171ba9d89e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f10a8e7ecfb47089c12f1033d511af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b323a7118c1f454298ee83a982626c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0e3c8a415044b4b83216b90f3c7f41",
            "max": 376873760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ac80028ce244c49b22ce171ba9d89e9",
            "value": 376873760
          }
        },
        "bbf46f9f7d6845a6abedad4349820bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d8c0c60a7904d56aa240db83d2d33b3",
              "IPY_MODEL_b323a7118c1f454298ee83a982626c8f",
              "IPY_MODEL_d121fbf04a634ae4a9719bea703f841e"
            ],
            "layout": "IPY_MODEL_9f10a8e7ecfb47089c12f1033d511af0"
          }
        },
        "bfa6aa9dea8245c6bdad25e9aea4c0ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1333139d5364c3e8bc3af2db8e215e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e9d71b02f049bc9c53fab95d4baa36",
            "max": 725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_639fa35f227d412e85ae938b742fb8b3",
            "value": 725
          }
        },
        "d121fbf04a634ae4a9719bea703f841e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccf9677fed24d10bab2ecd8d75a56a5",
            "placeholder": "​",
            "style": "IPY_MODEL_23c236739ab3462ab3fee4c412df6814",
            "value": " 377M/377M [00:03&lt;00:00, 188MB/s]"
          }
        },
        "ebad0d97d7bf48d4b9cf4f74050dcf33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
